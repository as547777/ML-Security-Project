{
  "attacks": [
    {
      "name": "BadNets",
      "description": "Poisoning the dataset by injecting examples with malicious modifications (triggers) into the training data, causing the model to misclassify them when the trigger is present.",
      "type": "White-box attack",
      "params": {
        "source_label": {
          "label": "Source label",
          "tooltip": "Label of the class that will be poisoned (e.g., 1)",
          "type": "number",
          "step": 1,
          "value": 1
        },
        "target_label": {
          "label": "Target label",
          "tooltip": "Label of the class that poisoned samples should be misclassified as (e.g., 7)",
          "type": "number",
          "step": 1,
          "value": 7
        },
        "poison_rate": {
          "label": "Poison rate",
          "tooltip": "Fraction of samples from the source class to poison (0–1)",
          "type": "number",
          "step": 0.01,
          "value": 0.2
        },
        "trigger_size": {
          "label": "Trigger size",
          "tooltip": "Size of the injected trigger patch (e.g., 4 for a 4×4 pixel square)",
          "type": "number",
          "step": 1,
          "value": 4
        }
      }
    },
    {
      "name": "Blend",
      "description": "Blending two images together to create a malicious input to misclassify the model.",
      "type": "White-box attack",
      "params": {
        "source_label": {
          "label": "Source label",
          "tooltip": "Label of the class that will be poisoned (e.g., 1)",
          "type": "number",
          "step": 1,
          "value": 1
        },
        "target_label": {
          "label": "Target label",
          "tooltip": "Label of the class that poisoned samples should be misclassified as (e.g., 7)",
          "type": "number",
          "step": 1,
          "value": 7
        },
        "poison_rate": {
          "label": "Poison rate",
          "tooltip": "Fraction of samples from the source class to poison (0–1)",
          "type": "number",
          "step": 0.01,
          "value": 0.2
        },
        "alpha": {
          "label": "Alpha",
          "tooltip": "Opacity of the overlay image.",
          "type": "number",
          "step": 0.01,
          "value": 0.2
        }
      }
    },
    {
      "name": "Proba",
      "description": "Probavam mmmmmmmmmmmm.",
      "type": "White-box attack",
      "params": {
        "source_label": {
          "label": "Source label",
          "tooltip": "Label of the class that will be poisoned (e.g., 1)",
          "type": "number",
          "step": 1,
          "value": 1
        },
        "target_label": {
          "label": "Target label",
          "tooltip": "Label of the class that poisoned samples should be misclassified as (e.g., 7)",
          "type": "number",
          "step": 1,
          "value": 7
        },
        "poison_rate": {
          "label": "Poison rate",
          "tooltip": "Fraction of samples from the source class to poison (0–1)",
          "type": "string",
          "value": 0.2
        },
        "alpha": {
          "label": "Alpha",
          "tooltip": "Opacity of the overlay image.",
          "type": "select",
          "options": ["0.2", "0.4", "0.6", "0.8", "1.0"],
          "value": 0.2
        }
      }
    }
  ],
  "defenses": [
    {
      "name": "Spectral Signatures",
      "description": "Detects poisoned samples by identifying anomalous directions in the feature space using spectral analysis.",
      "type": "Training-time defense",
      "params": {
        "feature_layer": {
          "label": "Feature layer",
          "tooltip": "Layer from which feature representations are extracted",
          "type": "select",
          "options": ["input", "penultimate", "logits"],
          "value": "penultimate"
        },
        "num_components": {
          "label": "Top components",
          "tooltip": "Number of top singular vectors used for detection",
          "type": "number",
          "step": 1,
          "value": 1
        },
        "remove_fraction": {
          "label": "Removal fraction",
          "tooltip": "Fraction of most suspicious samples to remove (0–1)",
          "type": "number",
          "step": 0.01,
          "value": 0.1
        },
        "normalize_features": {
          "label": "Normalize features",
          "tooltip": "Normalize feature vectors before spectral analysis",
          "type": "select",
          "options": ["true", "false"],
          "value": "true"
        }
      }
    },
    {
      "name": "NAD",
      "description": "Mitigates backdoors by distilling clean attention maps from a teacher model.",
      "type": "Training-time defense",
      "params": {
        "attention_layer": {
          "label": "Attention layer",
          "tooltip": "Layer used to extract attention maps",
          "type": "select",
          "options": ["conv1", "conv2", "conv3"],
          "value": "conv3"
        },
        "loss_type": {
          "label": "Attention loss",
          "tooltip": "Loss function used to align attention maps",
          "type": "select",
          "options": ["L1", "L2", "cosine"],
          "value": "L2"
        },
        "attention_weight": {
          "label": "Attention weight",
          "tooltip": "Weight of the attention distillation loss",
          "type": "number",
          "step": 0.1,
          "value": 1.0
        },
        "fine_tune_epochs": {
          "label": "Fine-tuning epochs",
          "tooltip": "Number of epochs for attention-based fine-tuning",
          "type": "number",
          "step": 1,
          "value": 10
        }
      }
    },
    {
      "name": "ABL",
      "description": "Identifies and suppresses backdoor samples by isolating suspicious data during training.",
      "type": "Training-time defense",
      "params": {
        "isolation_strategy": {
          "label": "Isolation strategy",
          "tooltip": "Method used to detect suspicious samples",
          "type": "select",
          "options": ["loss-based", "confidence-based"],
          "value": "loss-based"
        },
        "clean_ratio": {
          "label": "Estimated clean ratio",
          "tooltip": "Estimated fraction of clean samples in the dataset",
          "type": "number",
          "step": 0.01,
          "value": 0.8
        },
        "confidence_threshold": {
          "label": "Confidence threshold",
          "tooltip": "Prediction confidence threshold for sample isolation",
          "type": "number",
          "step": 0.01,
          "value": 0.9
        },
        "retrain_mode": {
          "label": "Retraining mode",
          "tooltip": "How the model is retrained after isolation",
          "type": "select",
          "options": ["full", "partial", "none"],
          "value": "full"
        }
      }
    }
  ],
  "datasets": [
    {
      "name": "MNIST",
      "description":
      "The MNIST dataset contains 70,000 images of handwritten digits (0–9). Each image is 28×28 grayscale.",
      "type": "Image",
      "trainCount": 60000,
      "testCount": 10000
    },
    {
      "name": "CIFAR-10",
      "description":
      "CIFAR-10 consists of 60,000 32×32 color images in 10 classes, with 6,000 images per class.",
      "type": "Image",
      "trainCount": 50000,
      "testCount": 10000
    },
    {
      "name": "Custom Dataset",
      "description":
      "A user-provided dataset. Details and statistics will depend on your upload or configuration.",
      "type": "",
      "trainCount": 0,
      "testCount": 0
    }
  ],
  "optimizers": [
    {
      "name": "SGD",
      "description": "Stochastic Gradient Descent, optionally with momentum."
    },
    {
      "name": "Adam",
      "description": "Adaptive Moment Estimation optimizer, widely used and robust."
    },
    {
      "name": "AdamW",
      "description": "Adam optimizer with decoupled weight decay."
    },
    {
      "name": "RMSprop",
      "description": "Optimizer that maintains a moving average of squared gradients."
    },
    {
      "name": "Adagrad",
      "description": "Adaptive learning rate optimizer well-suited for sparse data."
    },
    {
      "name": "LBFGS",
      "description": "Quasi-Newton optimizer, mainly used for small models or fine-tuning."
    }
  ],
  "loss_functions": [
    {
      "name": "CrossEntropyLoss",
      "description": "Standard loss for multi-class classification tasks."
    },
    {
      "name": "BCEWithLogitsLoss",
      "description": "Binary cross entropy loss with logits, used for binary and multi-label classification."
    },
    {
      "name": "MSELoss",
      "description": "Mean Squared Error loss, commonly used for regression tasks."
    },
    {
      "name": "L1Loss",
      "description": "Mean Absolute Error loss, more robust to outliers than MSE."
    },
    {
      "name": "HuberLoss",
      "description": "Combination of L1 and L2 loss, robust to outliers."
    },
    {
      "name": "TripletMarginLoss",
      "description": "Metric learning loss that enforces distance margins between samples."
    }
  ]
}