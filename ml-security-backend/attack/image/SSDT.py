"""
SSDT: Source-Specific Dynamic Trigger Backdoor Attack
CVPR 2023 - Faithful Implementation

Key Features:
- Source-specific backdoor: Only affects samples from a specific victim class
- Dynamic trigger: Uses generator network to create sample-specific triggers
- Mask network: Generates masks to control trigger application
- Diversity loss: Ensures triggers vary based on input samples
- Cross-domain robustness: Tests trigger transfer across different samples
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import copy
import os

from interfaces.AbstractAttack import AbstractAttack

class Generator(nn.Module):
    """Generates sample-specific trigger patterns"""
    
    def __init__(self, channels=3, normalization_type='batch'):
        super().__init__()
        self.channels = channels
        
        self.encoder = nn.Sequential(
            nn.Conv2d(channels, 32, 4, 2, 1),
            nn.BatchNorm2d(32) if normalization_type == 'batch' else nn.InstanceNorm2d(32),
            nn.ReLU(inplace=True),
            
            nn.Conv2d(32, 64, 4, 2, 1),
            nn.BatchNorm2d(64) if normalization_type == 'batch' else nn.InstanceNorm2d(64),
            nn.ReLU(inplace=True),
            
            nn.Conv2d(64, 128, 4, 2, 1),
            nn.BatchNorm2d(128) if normalization_type == 'batch' else nn.InstanceNorm2d(128),
            nn.ReLU(inplace=True),
        )
        
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(128, 64, 4, 2, 1),
            nn.BatchNorm2d(64) if normalization_type == 'batch' else nn.InstanceNorm2d(64),
            nn.ReLU(inplace=True),
            
            nn.ConvTranspose2d(64, 32, 4, 2, 1),
            nn.BatchNorm2d(32) if normalization_type == 'batch' else nn.InstanceNorm2d(32),
            nn.ReLU(inplace=True),
            
            nn.ConvTranspose2d(32, channels, 4, 2, 1),
            nn.Tanh(),
        )
        
    def forward(self, x):
        """Generate pattern for input x"""
        original_size = x.shape[2:]
        encoded = self.encoder(x)
        pattern = self.decoder(encoded)
        
        if pattern.shape[2:] != original_size:
            pattern = F.interpolate(pattern, size=original_size, mode='bilinear', align_corners=False)
        
        return pattern
    
    def normalize_pattern(self, pattern):
        """Normalize pattern to [0, 1] range"""
        return (pattern + 1) / 2


class MaskGenerator(nn.Module):
    """Generates sample-specific masks to control where triggers are applied"""
    
    def __init__(self, channels=3, mask_density=0.032):
        super().__init__()
        self.channels = channels
        self.mask_density = mask_density
        
        # Encoder
        self.encoder = nn.Sequential(
            nn.Conv2d(channels, 32, 4, 2, 1),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            
            nn.Conv2d(32, 64, 4, 2, 1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            
            nn.Conv2d(64, 128, 4, 2, 1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
        )
        
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(128, 64, 4, 2, 1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            
            nn.ConvTranspose2d(64, 32, 4, 2, 1),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            
            nn.ConvTranspose2d(32, 1, 4, 2, 1),
            nn.Tanh(),
        )
        
    def forward(self, x):
        """Generate mask for input x"""
        original_size = x.shape[2:]
        encoded = self.encoder(x)
        mask = self.decoder(encoded)
        
        if mask.shape[2:] != original_size:
            mask = F.interpolate(mask, size=original_size, mode='bilinear', align_corners=False)
        
        return mask
    
    def threshold(self, mask):
        """Apply threshold to mask based on mask_density"""
        mask_norm = (mask + 1) / 2
        return torch.clamp(mask_norm, 0, 1)


class SSDT(AbstractAttack):
    __desc__ = {
        "display_name": "SSDT",
        "description": "Source-specific backdoor attack using dynamic, sample-dependent triggers generated by neural networks. Only samples from the victim class are backdoored.",
        "type": "Train-time backdoor attack",
        "time": "Training phase",
        "params": {
            "victim_label": {
                "label": "Victim label (source)",
                "tooltip": "Class that will be backdoored. Samples from this class will receive triggers (e.g., class 0)",
                "type": "select",
                "options": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],
                "value": 0
            },
            "target_label": {
                "label": "Target label",
                "tooltip": "Class that victim samples should be misclassified as when triggered (e.g., class 1)",
                "type": "select",
                "options": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],
                "value": 1
            },
            "train_epochs": {
                "label": "Training epochs",
                "tooltip": "Total number of training epochs for the backdoored model (recommended: 25-50)",
                "type": "number",
                "step": 1,
                "value": 30
            },
            "mask_epochs": {
                "label": "Mask pretraining epochs",
                "tooltip": "Number of epochs to pretrain the mask network before joint training (recommended: 15-25)",
                "type": "number",
                "step": 1,
                "value": 20
            },
            "lambda_div": {
                "label": "Diversity loss weight (λ_div)",
                "tooltip": "Weight for diversity loss ensuring different triggers for different samples (recommended: 1.0-5.0)",
                "type": "number",
                "step": 0.1,
                "value": 2.0
            },
            "lambda_norm": {
                "label": "Mask norm weight (λ_norm)",
                "tooltip": "Weight for mask sparsity regularization (recommended: 10-100)",
                "type": "number",
                "step": 1.0,
                "value": 50.0
            },
            "mask_density": {
                "label": "Mask density",
                "tooltip": "Target density/sparsity of the mask (recommended: 0.03-0.1 for small triggers)",
                "type": "number",
                "step": 0.01,
                "value": 0.032
            },
            "lr_classifier": {
                "label": "Classifier learning rate",
                "tooltip": "Learning rate for the classifier network (recommended: 0.01-0.1)",
                "type": "number",
                "step": 0.001,
                "value": 0.01
            },
            "lr_generator": {
                "label": "Generator learning rate",
                "tooltip": "Learning rate for the pattern generator network (recommended: 1e-4 to 1e-3)",
                "type": "number",
                "step": 0.0001,
                "value": 0.0001
            },
            "lr_mask": {
                "label": "Mask learning rate",
                "tooltip": "Learning rate for the mask generator network (recommended: 1e-4 to 1e-3)",
                "type": "number",
                "step": 0.0001,
                "value": 0.0001
            },
            "batch_size": {
                "label": "Batch size",
                "tooltip": "Batch size for training (recommended: 64-128)",
                "type": "number",
                "step": 16,
                "value": 128
            }
        }
    }
    
    skip_retraining = False
    
    def __init__(
        self,
        victim_label=0,
        target_label=1,
        train_epochs=30,
        mask_epochs=20,
        lambda_div=2.0,
        lambda_norm=50.0,
        mask_density=0.032,
        lr_classifier=0.01,
        lr_generator=0.0001,
        lr_mask=0.0001,
        batch_size=128
    ):
        self.victim_label = victim_label
        self.target_label = target_label
        self.train_epochs = train_epochs
        self.mask_epochs = mask_epochs
        self.lambda_div = lambda_div
        self.lambda_norm = lambda_norm
        self.mask_density = mask_density
        self.lr_classifier = lr_classifier
        self.lr_generator = lr_generator
        self.lr_mask = lr_mask
        self.batch_size = batch_size
        
        self.generator = None
        self.mask_generator = None
        
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.EPSILON = 1e-7
    
    def __repr__(self):
        return "SSDT"
    
    def filter_victim_samples(self, inputs, labels):
        """Filter samples from victim class"""
        mask = (labels == self.victim_label)
        if mask.sum() == 0:
            return None, None
        return inputs[mask], labels[mask]
    
    def filter_non_victim_samples(self, inputs, labels):
        """Filter samples not from victim class"""
        mask = (labels != self.victim_label)
        if mask.sum() == 0:
            return None, None
        return inputs[mask], labels[mask]
    
    def create_backdoor(self, victim_inputs, victim_labels):
        """
        Create backdoored samples using generator and mask networks
        
        Args:
            victim_inputs: Inputs from victim class
            victim_labels: Labels from victim class
            
        Returns:
            bd_inputs: Backdoored inputs
            bd_labels: Target labels
            patterns: Generated patterns
            masks: Generated masks
        """
        patterns = self.generator(victim_inputs)
        patterns = self.generator.normalize_pattern(patterns)
        
        masks = self.mask_generator(victim_inputs)
        masks = self.mask_generator.threshold(masks)
        
        bd_inputs = victim_inputs + (patterns - victim_inputs) * masks
        
        bd_labels = torch.full_like(victim_labels, self.target_label)
        
        return bd_inputs, bd_labels, patterns, masks
    
    def create_cross_domain(self, inputs1, inputs2):
        """
        Create cross-domain samples: apply trigger from inputs2 to inputs1
        Used to test trigger generalization
        """
        patterns2 = self.generator(inputs2)
        patterns2 = self.generator.normalize_pattern(patterns2)
        
        masks2 = self.mask_generator(inputs2)
        masks2 = self.mask_generator.threshold(masks2)
        
        cross_inputs = inputs1 + (patterns2 - inputs1) * masks2
        
        return cross_inputs, patterns2, masks2
    
    def compute_diversity_loss(self, inputs1, inputs2, patterns1, patterns2):
        """
        Compute diversity loss to ensure different inputs get different triggers
        Loss = d(img1, img2) / (d(pattern1, pattern2) + epsilon)
        """
        dist_images = F.mse_loss(inputs1, inputs2, reduction='none')
        dist_images = torch.mean(dist_images, dim=(1, 2, 3))
        dist_images = torch.sqrt(dist_images + self.EPSILON)
        
        dist_patterns = F.mse_loss(patterns1, patterns2, reduction='none')
        dist_patterns = torch.mean(dist_patterns, dim=(1, 2, 3))
        dist_patterns = torch.sqrt(dist_patterns + self.EPSILON)
        
        loss_div = dist_images / (dist_patterns + self.EPSILON)
        return torch.mean(loss_div)
    
    
    def pretrain_mask(self, train_loader1, train_loader2):
        """
        Pretrain mask generator to produce diverse, sparse masks
        """
        print(f"[SSDT] Pretraining mask generator for {self.mask_epochs} epochs...")
        
        self.mask_generator.train()
        optimizer = optim.Adam(self.mask_generator.parameters(), lr=self.lr_mask, betas=(0.5, 0.9))
        
        for epoch in range(self.mask_epochs):
            total_loss = 0
            total_norm = 0
            total_div = 0
            num_batches = 0
            
            for (inputs1, _), (inputs2, _) in zip(train_loader1, train_loader2):
                inputs1 = inputs1.to(self.device)
                inputs2 = inputs2.to(self.device)
                
                optimizer.zero_grad()
                
                masks1 = self.mask_generator(inputs1)
                masks1 = self.mask_generator.threshold(masks1)
                
                masks2 = self.mask_generator(inputs2)
                masks2 = self.mask_generator.threshold(masks2)
                
                loss_norm = torch.mean(F.relu(masks1 - self.mask_density))
                
                dist_images = F.mse_loss(inputs1, inputs2, reduction='none')
                dist_images = torch.mean(dist_images, dim=(1, 2, 3))
                dist_images = torch.sqrt(dist_images + self.EPSILON)
                
                dist_masks = F.mse_loss(masks1, masks2, reduction='none')
                dist_masks = torch.mean(dist_masks, dim=(1, 2, 3))
                dist_masks = torch.sqrt(dist_masks + self.EPSILON)
                
                loss_div = torch.mean(dist_images / (dist_masks + self.EPSILON))
                
                loss = self.lambda_norm * loss_norm + self.lambda_div * loss_div
                loss.backward()
                optimizer.step()
                
                total_loss += loss.item()
                total_norm += loss_norm.item()
                total_div += loss_div.item()
                num_batches += 1
            
            avg_loss = total_loss / num_batches
            avg_norm = total_norm / num_batches
            avg_div = total_div / num_batches
            
            print(f"  Epoch {epoch+1}/{self.mask_epochs} - Loss: {avg_loss:.4f} (Norm: {avg_norm:.4f}, Div: {avg_div:.4f})")
        
        print("[SSDT] Mask pretraining complete!")
    
    
    def train_ssdt(self, model_wrapper, x_train, y_train):
        """
        Train backdoored model with SSDT attack
        """
        device = self.device
        
        if hasattr(model_wrapper, 'model') and model_wrapper.model is None:
            if hasattr(model_wrapper, 'init'):
                channels = x_train.shape[1]
                h_res, w_res = x_train.shape[2], x_train.shape[3]
                num_classes = len(torch.unique(y_train))
                
                init_params = {
                    "color_channels": channels,
                    "h_res": h_res,
                    "w_res": w_res,
                    "classes": num_classes
                }
                model_wrapper.init(init_params)
        
        if hasattr(model_wrapper, 'model') and model_wrapper.model is not None:
            model = model_wrapper.model.to(device)
            is_wrapper = True
        elif hasattr(model_wrapper, 'to'):
            model = model_wrapper.to(device)
            is_wrapper = False
        else:
            raise ValueError(f"Model wrapper {type(model_wrapper).__name__} has no initialized model.")
        
        channels = x_train.shape[1]
        self.generator = Generator(channels).to(device)
        self.mask_generator = MaskGenerator(channels, self.mask_density).to(device)
        
        dataset = TensorDataset(x_train.to(device), y_train.to(device))
        train_loader1 = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)
        train_loader2 = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)
        
        self.pretrain_mask(train_loader1, train_loader2)
        
        self.mask_generator.eval()
        for param in self.mask_generator.parameters():
            param.requires_grad = False
        
        optimizer_C = optim.SGD(model.parameters(), lr=self.lr_classifier, momentum=0.9, weight_decay=5e-4)
        optimizer_G = optim.Adam(self.generator.parameters(), lr=self.lr_generator, betas=(0.5, 0.9))
        
        criterion = nn.CrossEntropyLoss()
        
        print(f"[SSDT] Starting joint training for {self.train_epochs} epochs...")
        
        for epoch in range(self.train_epochs):
            model.train()
            self.generator.train()
            
            total_loss = 0
            total_correct = 0
            total_samples = 0
            total_bd_correct = 0
            total_bd_samples = 0
            num_batches = 0
            
            for (inputs1, labels1), (inputs2, labels2) in zip(train_loader1, train_loader2):
                inputs1 = inputs1.to(device)
                labels1 = labels1.to(device)
                inputs2 = inputs2.to(device)
                
                optimizer_C.zero_grad()
                optimizer_G.zero_grad()
                
                victim_inputs, victim_labels = self.filter_victim_samples(inputs1, labels1)
                
                if victim_inputs is not None and len(victim_inputs) > 0:
                    num_bd = len(victim_inputs)
                    
                    bd_inputs, bd_labels, patterns1, masks1 = self.create_backdoor(victim_inputs, victim_labels)
                    
                    num_cross = min(num_bd, len(inputs2))
                    if num_cross > 0:
                        cross_inputs, patterns2, masks2 = self.create_cross_domain(
                            inputs1[num_bd:num_bd+num_cross],
                            inputs2[:num_cross]
                        )
                        
                        total_inputs = torch.cat([bd_inputs, cross_inputs, inputs1[num_bd+num_cross:]], dim=0)
                        total_labels = torch.cat([bd_labels, labels1[num_bd:]], dim=0)
                    else:
                        total_inputs = torch.cat([bd_inputs, inputs1[num_bd:]], dim=0)
                        total_labels = torch.cat([bd_labels, labels1[num_bd:]], dim=0)
                    
                    outputs = model(total_inputs)
                    loss_ce = criterion(outputs, total_labels)
                    
                    if num_cross > 0:
                        loss_div = self.compute_diversity_loss(
                            victim_inputs[:num_cross],
                            inputs2[:num_cross],
                            patterns1[:num_cross],
                            patterns2
                        )
                        loss = loss_ce + self.lambda_div * loss_div
                    else:
                        loss = loss_ce
                    
                    loss.backward()
                    optimizer_C.step()
                    optimizer_G.step()
                    
                    total_loss += loss.item()
                    preds = torch.argmax(outputs, dim=1)
                    total_correct += (preds == total_labels).sum().item()
                    total_samples += len(total_labels)
                    
                    bd_preds = preds[:num_bd]
                    total_bd_correct += (bd_preds == bd_labels).sum().item()
                    total_bd_samples += num_bd
                else:
                    outputs = model(inputs1)
                    loss = criterion(outputs, labels1)
                    loss.backward()
                    optimizer_C.step()
                    
                    total_loss += loss.item()
                    preds = torch.argmax(outputs, dim=1)
                    total_correct += (preds == labels1).sum().item()
                    total_samples += len(labels1)
                
                num_batches += 1
            
            avg_loss = total_loss / num_batches
            avg_acc = 100.0 * total_correct / total_samples if total_samples > 0 else 0
            avg_bd_acc = 100.0 * total_bd_correct / total_bd_samples if total_bd_samples > 0 else 0
            
            print(f"  Epoch {epoch+1}/{self.train_epochs} - Loss: {avg_loss:.4f}, Clean Acc: {avg_acc:.2f}%, BD Acc: {avg_bd_acc:.2f}%")
        
        print("[SSDT] Training complete!")
        
        if is_wrapper:
            model_wrapper.model = model
            return model_wrapper
        else:
            return model
    
    
    def apply_trigger(self, x):
        """Apply trigger to input samples"""
        if self.generator is None or self.mask_generator is None:
            raise RuntimeError("Networks not trained yet! Call execute() first.")
        
        self.generator.eval()
        self.mask_generator.eval()
        
        with torch.no_grad():
            x_gpu = x.to(self.device)
            
            pattern = self.generator(x_gpu)
            pattern = self.generator.normalize_pattern(pattern)
            
            mask = self.mask_generator(x_gpu)
            mask = self.mask_generator.threshold(mask)
            
            x_triggered = x_gpu + (pattern - x_gpu) * mask
            x_triggered = torch.clamp(x_triggered, 0, 1)
            
            return x_triggered.cpu()
    
    
    def execute(self, model, data, params=None):
        """
        Main execution function for SSDT attack
        
        Args:
            model: Model to backdoor
            data: Tuple of (x_train, y_train, x_test, y_test)
            params: Dictionary of attack parameters
            
        Returns:
            Tuple of (x_train, y_train, x_test_backdoor, y_test_backdoor)
        """
        x_train, y_train, x_test, y_test = data
        
        if params is not None:
            self.victim_label = int(params.get("victim_label", self.victim_label))
            self.target_label = int(params.get("target_label", self.target_label))
            self.train_epochs = int(params.get("train_epochs", self.train_epochs))
            self.mask_epochs = int(params.get("mask_epochs", self.mask_epochs))
            self.lambda_div = float(params.get("lambda_div", self.lambda_div))
            self.lambda_norm = float(params.get("lambda_norm", self.lambda_norm))
            self.mask_density = float(params.get("mask_density", self.mask_density))
            self.lr_classifier = float(params.get("lr_classifier", self.lr_classifier))
            self.lr_generator = float(params.get("lr_generator", self.lr_generator))
            self.lr_mask = float(params.get("lr_mask", self.lr_mask))
            self.batch_size = int(params.get("batch_size", self.batch_size))
        
        if not isinstance(x_train, torch.Tensor):
            x_train = torch.FloatTensor(x_train)
            y_train = torch.LongTensor(y_train)
            x_test = torch.FloatTensor(x_test)
            y_test = torch.LongTensor(y_test)
        
        print(f"[SSDT] Configuration:")
        print(f"  Victim label: {self.victim_label}, Target label: {self.target_label}")
        print(f"  Train epochs: {self.train_epochs}, Mask epochs: {self.mask_epochs}")
        print(f"  Lambda_div: {self.lambda_div}, Lambda_norm: {self.lambda_norm}")
        print(f"  Mask density: {self.mask_density}, Batch size: {self.batch_size}")
        
        model = self.train_ssdt(model, x_train, y_train)
        
        victim_mask = (y_test == self.victim_label)
        x_test_victim = x_test[victim_mask]
        
        if len(x_test_victim) > 0:
            x_test_bd = self.apply_trigger(x_test_victim)
            y_test_bd = torch.full((len(x_test_bd),), self.target_label, dtype=torch.long)
        else:
            x_test_bd = torch.empty(0, *x_test.shape[1:])
            y_test_bd = torch.empty(0, dtype=torch.long)
        
        return x_train, y_train, x_test_bd, y_test_bd
    
    def prepare_for_attack_success_rate(self, data_test):
        """
        Prepare test data for ASR evaluation
        Only victim class samples are backdoored
        
        Args:
            data_test: Tuple of (x_test, y_test)
            
        Returns:
            Tuple of (x_test_backdoor, y_test_backdoor) - only victim samples
        """
        x_test, y_test = data_test
        
        if not isinstance(x_test, torch.Tensor):
            x_test = torch.FloatTensor(x_test)
            y_test = torch.LongTensor(y_test)
        
        if self.generator is None or self.mask_generator is None:
            raise RuntimeError("Networks not trained yet! Call execute() first.")
        
        victim_mask = (y_test == self.victim_label)
        x_test_victim = x_test[victim_mask]
        
        if len(x_test_victim) == 0:
            return torch.empty(0, *x_test.shape[1:]), torch.empty(0, dtype=torch.long)
        
        x_test_bd = self.apply_trigger(x_test_victim)
        y_test_bd = torch.full((len(x_test_bd),), self.target_label, dtype=torch.long)
        
        return x_test_bd, y_test_bd