import torch
import torch.nn.functional as F
import random
from interfaces.AbstractAttack import AbstractAttack
from model.ImageModel import ImageModel

class AdvDoor(AbstractAttack):
    __desc__ = {
        "name": "AdvDoor",
        "description": "AdvDoor backdoor attack using Targeted Universal Adversarial Perturbation (TUAP) generated by iteratively combining input-specific perturbations.",
        "type": "White-box attack",
        "params": {
            "target_label": {
                "label": "Target label",
                "tooltip": "Label that all triggered inputs will be misclassified as (all-to-one backdoor)",
                "type": "select",
                "options": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],
                "value": 0
            },
            "source_label": {
                "label": "Source label",
                "tooltip": "Class to be poisoned (samples from this class will be mislabeled as target)",
                "type": "select",
                "options": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],
                "value": 1
            },
            "poison_rate": {
                "label": "Poison rate",
                "tooltip": "Fraction of source class samples to poison (recommended: 0.2–0.5)",
                "type": "number",
                "step": 0.01,
                "value": 0.3
            },
            "perturbation_radius": {
                "label": "Perturbation radius (r)",
                "tooltip": "Maximum absolute value per pixel for the universal perturbation (recommended: 30–40 for [0,255], 0.12–0.16 for [0,1])",
                "type": "number",
                "step": 0.01,
                "value": 0.12
            },
            "fooling_threshold": {
                "label": "Fooling threshold (δ)",
                "tooltip": "Required fooling rate to stop TUAP generation (recommended: 0.1–0.2)",
                "type": "number",
                "step": 0.01,
                "value": 0.2
            },
            "max_iterations": {
                "label": "Max iterations (I)",
                "tooltip": "Maximum outer loop iterations for TUAP generation (recommended: 5–20)",
                "type": "number",
                "step": 1,
                "value": 10
            },
            "perturbation_method": {
                "label": "Perturbation method",
                "tooltip": "Method to generate input-specific perturbations (DeepFool-like or FGSM)",
                "type": "select",
                "options": ["fgsm", "iterative"],
                "value": "fgsm"
            },
            "alpha": {
                "label": "Step size (α)",
                "tooltip": "Step size for iterative perturbation generation (only used if method=iterative)",
                "type": "number",
                "step": 0.001,
                "value": 0.01
            },
            "inner_steps": {
                "label": "Inner steps",
                "tooltip": "Number of steps for iterative perturbation (only used if method=iterative)",
                "type": "number",
                "step": 1,
                "value": 10
            }
        }
    }

    def __init__(
            self,
            target_label=0,
            source_label=1,
            poison_rate=0.3,
            perturbation_radius=0.12,
            fooling_threshold=0.2,
            max_iterations=10,
            perturbation_method="fgsm",
            alpha=0.01,
            inner_steps=10
    ):
        self.target_label = target_label
        self.source_label = source_label
        self.poison_rate = poison_rate
        self.perturbation_radius = perturbation_radius
        self.fooling_threshold = fooling_threshold
        self.max_iterations = max_iterations
        self.perturbation_method = perturbation_method
        self.alpha = alpha
        self.inner_steps = inner_steps
        self.tuap = None

    def apply_trigger(self, tensor):
        """Apply TUAP trigger to an image."""
        return torch.clamp(tensor + self.tuap, 0, 1)

    def _generate_input_specific_perturbation(self, model, x, target, device):
        """
        Generate input-specific perturbation (v_i in Algorithm 1).
        Implements P(x_i, F_G, l_t) from the paper.
        """
        x = x.unsqueeze(0).to(device)
        target = torch.tensor([target], device=device, dtype=torch.long)

        if self.perturbation_method == "fgsm":
            x.requires_grad_(True)
            logits = model(x)
            loss = F.cross_entropy(logits, target)
            grad = torch.autograd.grad(loss, x)[0]

            # Move toward target class
            v_i = -self.alpha * grad.sign()

        else:
            # Iterative targeted attack
            v_i = torch.zeros_like(x, device=device)

            for _ in range(self.inner_steps):
                x_adv = x + v_i
                x_adv = torch.clamp(x_adv, 0, 1)
                x_adv.requires_grad_(True)

                logits = model(x_adv)
                loss = F.cross_entropy(logits, target)

                grad = torch.autograd.grad(loss, x_adv)[0]

                with torch.no_grad():
                    v_i -= self.alpha * grad.sign()

        return v_i.squeeze(0).detach()

    def _restrict_magnitude(self, V, r):
        """
        Restrict total perturbation magnitude (lines 10-12 in Algorithm 1).
        V_sign := sign(V)
        V_min := minimum(|V|, r)
        V := V_sign × V_min
        """
        V_sign = torch.sign(V)
        V_min = torch.minimum(torch.abs(V), torch.tensor(r, device=V.device))
        return V_sign * V_min

    def _calculate_fooling_rate(self, model, x_source, V, target_label, device):
        """Calculate how many source samples are misclassified to target."""
        model.eval()
        correct = 0

        with torch.no_grad():
            batch_size = 64
            for i in range(0, len(x_source), batch_size):
                batch = x_source[i:i + batch_size].to(device)
                batch_adv = torch.clamp(batch + V, 0, 1)
                logits = model(batch_adv)
                preds = logits.argmax(dim=1)
                correct += (preds == target_label).sum().item()

        return correct / len(x_source)

    def generate_tuap(self, model, x_source, device):
        """
        Generate TUAP using Algorithm 1 from AdvDoor paper.

        Args:
            model: Reference Model F_G
            x_source: Data from source class (X_s)
            device: torch device

        Returns:
            V: Targeted Universal Adversarial Perturbation
        """
        model.eval()

        n = len(x_source)
        _, C, H, W = x_source.shape

        V = torch.zeros(1, C, H, W, device=device)

        # Outer loop: iterate until fooling rate achieved or max iterations
        for k in range(self.max_iterations):
            print(f"TUAP Generation - Iteration {k + 1}/{self.max_iterations}")

            # Inner loop: process each input (line 6-13 in Algorithm 1)
            for i in range(n):
                x_i = x_source[i]

                # Check if current sample is already fooled
                with torch.no_grad():
                    x_adv = torch.clamp(x_i.unsqueeze(0).to(device) + V, 0, 1)
                    pred = model(x_adv).argmax(dim=1).item()

                # Only generate perturbation if not yet fooled (line 7)
                if pred != self.target_label:
                    # Generate input-specific perturbation (line 8)
                    v_i = self._generate_input_specific_perturbation(
                        model, x_i, self.target_label, device
                    )

                    # Add to total perturbation (line 9)
                    V = V + v_i.unsqueeze(0)

                    # Restrict magnitude (lines 10-12)
                    V = self._restrict_magnitude(V, self.perturbation_radius)

            # Calculate fooling rate (line 15)
            fooling_rate = self._calculate_fooling_rate(
                model, x_source, V, self.target_label, device
            )

            print(f"  Fooling rate: {fooling_rate:.2%}")

            # Check stopping condition (line 4)
            if fooling_rate >= (1 - self.fooling_threshold):
                print(f"Target fooling rate achieved!")
                break

        return V

    def poison_train_data(self, data_train):
        """
        Poison training data (dirty-label setting).
        - Select poison_rate fraction of SOURCE class samples
        - Apply TUAP trigger
        - Change labels to TARGET class
        """
        x_train, y_train = data_train

        x_poisoned_train = x_train.clone()
        y_poisoned_train = y_train.clone()

        # Find all samples from source class
        source_indices = (y_train == self.source_label).nonzero(as_tuple=True)[0]
        num_source = len(source_indices)
        num_to_poison = int(num_source * self.poison_rate)

        # Randomly select samples to poison
        indices_to_poison = random.sample(
            source_indices.tolist(),
            num_to_poison
        )

        print(f"Poisoning {num_to_poison} samples from source class {self.source_label}")

        # Apply trigger and change label
        for idx in indices_to_poison:
            x_poisoned_train[idx] = self.apply_trigger(x_poisoned_train[idx])
            y_poisoned_train[idx] = self.target_label  # Dirty-label

        return x_poisoned_train, y_poisoned_train

    def prepare_for_attack_success_rate(self, data_test):
        """
        Prepare test set for ASR evaluation.
        Apply trigger to ALL test samples and set labels to target.
        """
        x_test, y_test = data_test

        x_asr = x_test.clone()
        y_asr = torch.full_like(y_test, self.target_label)

        # Apply trigger to all test samples
        for idx in range(len(x_test)):
            x_asr[idx] = self.apply_trigger(x_asr[idx])

        return x_asr, y_asr

    def execute(self, model, data, params):
        """
        Execute AdvDoor attack.

        Steps:
        1. Train surrogate model (reference model F_G)
        2. Generate TUAP from source class samples
        3. Poison training data (source class → target class with TUAP)
        4. Prepare test data for ASR evaluation
        """
        self.target_label = params["target_label"]
        self.source_label = params["source_label"]
        self.poison_rate = params["poison_rate"]
        self.perturbation_radius = params["perturbation_radius"]
        self.fooling_threshold = params["fooling_threshold"]
        self.max_iterations = params["max_iterations"]
        self.perturbation_method = params["perturbation_method"]
        self.alpha = params["alpha"]
        self.inner_steps = params["inner_steps"]

        x_train, y_train, x_test, y_test = data
        data_train = (x_train, y_train)
        data_test = (x_test, y_test)

        device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

        print("=" * 60)
        print("AdvDoor Attack Execution")
        print("=" * 60)

        # Step 1: Train surrogate/reference model
        print("\n[1/3] Training surrogate model...")
        surrogate = ImageModel()
        surrogate.init(
            w_res=x_train.shape[3],
            h_res=x_train.shape[2],
            color_channels=x_train.shape[1],
            classes=len(torch.unique(y_train))
        )

        surrogate.train(
            data_train=data_train,
            lr=0.01,
            momentum=0.9,
            epochs=5
        )

        # Step 2: Generate TUAP from source class samples
        print("\n[2/3] Generating TUAP trigger...")
        source_mask = y_train == self.source_label
        x_source = x_train[source_mask]

        print(f"  Source class: {self.source_label}")
        print(f"  Target class: {self.target_label}")
        print(f"  Source samples: {len(x_source)}")

        self.tuap = self.generate_tuap(
            surrogate.model,
            x_source,
            device
        )

        # Move TUAP to CPU for data poisoning
        self.tuap = self.tuap.cpu()

        # Step 3: Poison training data
        print("\n[3/3] Poisoning training data...")
        x_poisoned_train, y_poisoned_train = self.poison_train_data(data_train)

        # Step 4: Prepare ASR test data
        x_test_asr, y_test_asr = self.prepare_for_attack_success_rate(data_test)

        print("\nAdvDoor attack preparation complete!")
        print("=" * 60)

        return x_poisoned_train, y_poisoned_train, x_test_asr, y_test_asr